= LD preload 

== Задачка
Задача: есть программа на тысячу строк. Заказчик считает, что программа работает медленно. Нам нужно ускорить данную программу на столько, на сколько это возможно. Или сказать, что это невозможно.

Что будем делать? 

* профилируем. `valgring` -- профилирует системные вызовы и любые ассемблерные инструкции процессору проводит через себя. Отобразим его результат графически `KCachegrind`

Видим, что больше всего времени занимает генерация случайных чисел. Хотим заменить на стандартное. Тратим человекоминуту и улучшаем на наносекунду.

Скомпилировали со всеми возможными флагами оптимизации и увеличили на 0.05%.

* Смотрим на код, параллелим цикл, который работает константное число раз. У нас 16 адрес -- становится на 3-4 тысячных процента лучше.

* Практический совет: удалить весь код с логикой и посмотреть, сколько программа работает без него. Ускорим на одну сотую долю процента!

* `time ./diff` 

Выдеёт 3 значение 

** real -- атсрономическое время 
** user -- процессорное время -- (умноженное на количество процессоров), выпполняющееся в user-space 
** sys -- то же самое в kernel-space 

Результаты: 

* real : 3,83s 
* user : 0,83s 
* sys : 0,0s

Вывод: чего-то ждем. Пощищем код. Нашлиsleep(1000)! Его не было в профилировщике, т.к. он смотрел на процессорное время, а sleep его не занимает. Это ограничение valgring. А еще он фактически однопоточный. 

С помощью команды time мы поняли, что `bottle neck` не в процессорном времени. 

=== strace ./foo 
Мы смогли бы найти, что mutex ждет какое-то время и системные вызовы (но Рита уверена).

== О профилировщиках 
Бывают профилировщики, которые используют разные инстурменты. Самые полезные -- по процессорному времени и блокировкам (в частности, ввода/вывода).

Производительность: valgring может замедлить до 120 раз (от минуты до двух часов). Это крайне дорого для промышленной разработке.

Семплирующие: обычно десятки процентов.

* valgrind -- почти не используется в проде, лучше PeRV
* JMC -- java mission(?) control for jvm
* profiler in IDE
* perf -- для производительности

=== Семплирующие профилировщики 
Не являются VM 
Периодически делают снимки программы, собирая статистики

* parallel studio 
* vtune 
* PeRV 


== Рантайм загрузчик 

=== Пример: проблема одинаковых сигнатур функций
Две команды независимо пишут код 

a.cpp
```cpp 
//не объявлена в заголовке
foo(){
    std::cout << "a";
}

//объявлена в заголовке!!
do_a(){
    foo();
}
```

b.cpp
```cpp 
//не объявлена в заголовке
foo(){
    std::cout << "b";
}

//объявлена в заголовке!!
do_b(){
    foo();
}
```

a компилируется в a.so, b компилируется в b.so

main.cpp  
```cpp 
int main(){
    do_a();
    do_b();
}
```

`g++ main.cpp -lb -la`

Что произойдет? 

* соберется 
* не соберется, будет рантайм ошибка 
* Выведем в консоль 
** a b 
** a a
** b b
** b a   

Правильный ответ: `b b`

=== Загрузчик исполняемого файла 

Когда пишем ./a.out вызывается runtime загрущик (загрузчик исполняемого файла)

* берет исполняемый файл и грузит в оп 
* берет LDD и ищет какие динамические библиотеки ему нужно подгрузить, чтобы программа работала (рекурсивно)

Если библиотеки находятся на одном уровне (не зависят друг от друга, как а б). то они будут загружены в том же порядке, в котором указаны в команде 

У каждого elf файла есть таблица разделяемый символов, в которых определены используемые функции и переменные. Там их замангленные имена. Есть столбец, где указано -- этот символ определен в этом эльфе или его нужно загружать из другого. Символы могут быть двух видом.

Если не определены в моем файле -- то определены как WEAK (слабое связывание). Её код нужно искать в другом месте.

Если global -- то функция и объяалена и определена в текущем исполняемом файле. 

Загрузчик формирует таблицу, в которой слева -- все символы, а справа -- их адреса в памяти. Заполнение этой таблицы как происходит? 

1. Если в этом файле -- просто пишем его. 
2. Если его нет -- у всех файлов, которые были указаны при линковке ищем его первое вхождение.

Когда идет исполнение кода, любое обращение к внешним символам происходит через эту таблицу. 

Ошибка ли это? Да, с точки зрения дизайна кода. Как исправить? 

* Добавить функцию в анонимный неймспейс -- в качестве его имени сгенерируется случайная строка и имена будут разные. Но такая случайность не подходит для спецификации кода (контрольная сумма кода будет разная!). Чтобы это поправить -- в компилятор можно передать чид для генератора случайных строк, чтобы генерация была одинаковой.
* Добавить функции ключевое слово static 

== LD  
=== Пример: 
Есть программа, которая куда-то пишет. Делаем свою разделяемую библиотеку. Делаем в ней функцию, которая дублирует сигнатуру функции write.

spy.so
```cpp
write(...){
    send_data(GRU);
    //вызвать конретный write из конкретной библиотеки с конкретными параметрами 
} 
```

LD_PRELOAD = srt.so

Теперь при загрузке любых библиотек первым будет загружаться spy.so и все функции сперва будут искаться в ней. Поэтому мы сможем подменить стандартные функции. И, например, воровать все данные из write и отправлять их себе на почту.

На основе LD_PRELOAD работает часть семплирующих профилировщиков: все syscall оборачиваются функциями профилировщика. Поэтому программы не нужно перекомпилировать. 

Если нет системных вызовов? Профилировщик будет собирать информацию с процессора и т.д.

Зачем эти фугкции в профилировщике? Теперь при любом вызове функции мы можем делать снепшот стека вызовов и т.д.

В какой области запускаем его? В user space, в kernel space -- может и можно, но инфраструктурно это очень затратное мероприятие.


== Vtune (usually for Intel)

=== Hotspots -- время процессора

то же что valgring 

image::12/2024-01-23-20-08-17.png[]

=== threading 
image::12/2024-01-23-20-08-54.png[]

Нам сразу говорят, что эффективное время процессора 0.05% 

В графовом предаствлении непроцессорные вещи тоже указаны (sleep)

image::12/2024-01-23-20-10-37.png[]

==== Timeline 
Временная шкала со всеми событиями данного потока.

image::12/2024-01-23-20-12-39.png[]

=== Performance snapshot 

Показывает какие методы профилирования потенциально могут быть полезны для данного кода

image::12/2024-01-23-20-11-33.png[]

=== Time interval  
Часто у приложений есть время загрузки данных или периоды пиковой нагрузки процессора. Поэтому бывает интересно смотреть только на ту часть интервала, которое не удовлетворяет SLA. Можно вывести статистику только по этому участку. 

image::12/2024-01-23-20-15-16.png[]

 SLA - service layer agreedment -- соглашение об уровне сервиса (на какую нагрузку гарантированно способно наше приложение).

=== Многопоточность 
Время исполнения/ожидания для каждого потока
image::12/2024-01-23-20-16-28.png[]

Взаимодействие через потоки через примитивы синхронизации 

image::12/2024-01-23-20-17-21.png[]

Масштаб побольше

image::12/2024-01-23-20-18-42.png[]

Через этот инструмент можно проверить, что приложение ведет себя так, как было задумано. Например, что нет всегда спящих потоков.

== Профилирования работы с памятью  (memory access)
Это редкий случай. Его не имеет смысл проверять, если не были проведены предыдущие способы профилировки.

Программа:

image::12/2024-01-23-20-56-04.png[]

image::12/2024-01-23-20-57-21.png[]

Функция суммирует все элементы матриц. Первая сначала по строчкам, затем по столбцам. Вторая -- наоборот. 

=== Эксперименты
Скомпилировали в дебаге без оптимизаций.

==== Колонки  (плохое) в debug (-O0 -g)

image::12/cols-debug.png[]

Заняло 5 секунд

==== Строчки debug

image::12/lines-debug.png[]

==== Колонки release (-O3)

image::12/cols-release%20.png[]

==== Строчки debug 
image::12/lines-release.png[]


== Метрики Memory Access 

Как следует интерпретировать результаты?

* LLC Miss Count -- last (?) cache. Имеет смысл в рамках одного кванта планирования ядра ОС.

Сравним результаты у разных замеров 

Как сравнивать: 

image::12/compare_results.png[] 

Результаты сравнения: 

image::12/compare_release.png[]

=== Cache miss & bound
Cache miss у строчек вообще нет. Из высоких cache miss у второго, вытекают высокие `bound`. 

  bound - как долго процессорное время тратится здесь. Чем меньше - тем лучше.

  Почему не трогаем L2? Говорят, L1 и L3 работают в паре

Хотим посмотреть в какой строчке cache miss происходит чаще всего. Для этого нужно включить дополнительные галочки.

image::12/miss_statistic.png[]

===  CPI Rate 
Показывает насколько потенциально можно ускорить приложения, не меняя алгоритм, а эффективнее расходовать память 

image::12/cpi_rate.png[75%]

За один цикл процессора мб сделано 4 инструкции процессора (определяется архитектурой). Если программа написана так, что в каждый такт все эти 4 инструкции будут сразу выполняться (все данные уже лежат в кешах!), то метрика будет идеальной.

Идеальное значение метрики -- 0.25, это 0.25 цикла на 1 инструкцию. Если она больше 1 -- то есть явные проблемы с доступом памяти в приложении. Значит можно сравнительно небольшими усилиями понять где это происходит (где cache miss) и реорганизовать код.

== Зачем профилировать? 
* Увеличить производительность.
* Значительно сэкономить ресурсы.
* Еще бывает профилирование энергопотребления. Это нужно для мобильных приложений и их виртуальных машин.


Можно профилировать, запоминая данные из CI. Добавлять само профилирование в CI обычно не целесообразно, очень много ресурсов потребляет.

Коридор производительности -- производительность тестов не должна меняться на x процентов от запуска к запуску.

== Утечки памяти 
Профилирование -- это анализ производительности. 

Утечки памяти -- это анализ памяти, он обычно выполняется сторонними утилитами. Сейчас эту функцию выполняет санитайзер, который обычно встроен в компилятор. Но можно использовать memcheck valgrind. 

Для jvm можно раз в час снимать снепшоты и смотреть на то, какие объекты появились.